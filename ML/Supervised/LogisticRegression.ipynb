{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b401d58a-732c-4133-8f8a-1b55e0fc582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6a56edd-36df-4f25-9066-6085f013d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kaggle\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Never Classification Metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7211904-288d-430b-b7f5-4ab9543725ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify this path according to your system path : Where you kept ur python notebook file\n",
    "# MySelf\n",
    "data = \"/Users/sanjayk/Dropbox/Work/Careerera/Final/ML/data/\"\n",
    "#downloading the headbrain.csv file\n",
    "kaggle.api.authenticate()\n",
    "kaggle.api.dataset_download_files(\"nancyalaswad90/breast-cancer-dataset\", unzip=True)   \n",
    "#kaggle.api.dataset_download_files(\"nancyalaswad90/breast-cancer-dataset\", unzip=True,path=data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121897b2-09de-41eb-82dd-081ac865bf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cancer.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45bc7413-4e3a-4019-b6c5-a0983f066642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset : (569, 32)\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of the dataset : {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad573d99-764d-4bb1-aa14-934279072cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name                   :     Null Values :   Unique Values\n",
      "**********************************************************************\n",
      "id                             :               0 :             569\n",
      "diagnosis                      :               0 :               2\n",
      "radius_mean                    :               0 :             456\n",
      "texture_mean                   :               0 :             479\n",
      "perimeter_mean                 :               0 :             522\n",
      "area_mean                      :               0 :             539\n",
      "smoothness_mean                :               0 :             474\n",
      "compactness_mean               :               0 :             537\n",
      "concavity_mean                 :               0 :             537\n",
      "concave_points_mean            :               0 :             542\n",
      "symmetry_mean                  :               0 :             432\n",
      "fractal_dimension_mean         :               0 :             499\n",
      "radius_se                      :               0 :             540\n",
      "texture_se                     :               0 :             519\n",
      "perimeter_se                   :               0 :             533\n",
      "area_se                        :               0 :             528\n",
      "smoothness_se                  :               0 :             547\n",
      "compactness_se                 :               0 :             541\n",
      "concavity_se                   :               0 :             533\n",
      "concave_points_se              :               0 :             507\n",
      "symmetry_se                    :               0 :             498\n",
      "fractal_dimension_se           :               0 :             545\n",
      "radius_worst                   :               0 :             457\n",
      "texture_worst                  :               0 :             511\n",
      "perimeter_worst                :               0 :             514\n",
      "area_worst                     :               0 :             544\n",
      "smoothness_worst               :               0 :             411\n",
      "compactness_worst              :               0 :             529\n",
      "concavity_worst                :               0 :             539\n",
      "concave_points_worst           :               0 :             492\n",
      "symmetry_worst                 :               0 :             500\n",
      "fractal_dimension_worst        :               0 :             535\n"
     ]
    }
   ],
   "source": [
    "print('{:30} : {:>15} : {:>15}' .format('Feature Name', 'Null Values', 'Unique Values'))\n",
    "print('*'*70)\n",
    "for name in df.columns:\n",
    "    print(f'{name:30} : {df[name].isna().sum():15} : {df[name].nunique():15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6373b59e-d81c-4d91-a422-2ebcdd59bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the id column from data frame\n",
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbdf142-143a-405c-9337-19087f3184f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to convert  a Ordinal output category "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f54bc75a-104d-4587-a3ba-cdec766b71c0",
   "metadata": {},
   "source": [
    "# 1. Panda Mapping Function\n",
    "df['diagnosis'] = df['diagnosis'].map({'M':  0, 'B': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04357dbc-827b-46f0-ba06-2e35d0025a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "564    1\n",
       "565    1\n",
       "566    1\n",
       "567    1\n",
       "568    0\n",
       "Name: diagnosis, Length: 569, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Label Encoder from Scikit Learn Module\n",
    "ylabel = df['diagnosis']\n",
    "le =  LabelEncoder()\n",
    "le.fit(ylabel)\n",
    "df['diagnosis'] = le.transform(ylabel)\n",
    "df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4f0fc0-c5cd-4e33-9f10-6eda9132707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent Variables\n",
    "X = df.drop('diagnosis',axis=1)\n",
    "\n",
    "# dependent Variables\n",
    "y =  df[['diagnosis']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea081ae-7831-41ef-a998-eab51292776f",
   "metadata": {},
   "source": [
    "## Different Type of Sampling to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e630ae80-2013-4f01-a45a-9490b8ba0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling to choose training and testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=42)\n",
    "\n",
    "# Stratified Sampling to choose training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=54,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c40ac3-d466-46c5-a178-ec3f745b6199",
   "metadata": {},
   "source": [
    "#help(GridSearchCV)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc198d9-cf25-4eb7-b8dd-5573dd55add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter : {'C': 10.0, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "Best Estimator : LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'penalty': ['l1', 'l2', 'elasticnet'], \n",
    "              'solver': ['lbfgs', 'liblinear','newton-cg','sag','saga'],\n",
    "              'C' : [ 0.1, 1.0, 10.0],\n",
    "              'tol' : [ 0.00001, 0.0001, 0.001 ],\n",
    "              'max_iter' : [100, 1000] \n",
    "             }\n",
    "\n",
    "clf = LogisticRegression()\n",
    "gs = GridSearchCV(clf, parameters)\n",
    "gs.fit(X,y)\n",
    "\n",
    "# Get the best parameters and estimator\n",
    "print(f'Best Parameter : {gs.best_params_}')\n",
    "print(f'Best Estimator : {gs.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8cc24-5c7f-4d44-90dc-530185b2dcd7",
   "metadata": {},
   "source": [
    "## Best Parameters for any Classifier Kernel for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f848b50-c874-49c1-809e-bf99492de8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logisitic Regression : 97.66%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=54, stratify=y)\n",
    "\n",
    "#Changing the random state means: Changing Different Training and Testing Samples\n",
    "clf = LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Accuracy of Logisitic Regression : {accuracy_score(y_pred,y_test)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fceb25d8-3d20-4ef3-adf8-059f04ef6c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>14.480</td>\n",
       "      <td>21.46</td>\n",
       "      <td>94.25</td>\n",
       "      <td>648.2</td>\n",
       "      <td>0.09444</td>\n",
       "      <td>0.09947</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.049380</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.05636</td>\n",
       "      <td>...</td>\n",
       "      <td>16.210</td>\n",
       "      <td>29.25</td>\n",
       "      <td>108.40</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.19760</td>\n",
       "      <td>0.33490</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.06846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>13.770</td>\n",
       "      <td>13.27</td>\n",
       "      <td>88.06</td>\n",
       "      <td>582.7</td>\n",
       "      <td>0.09198</td>\n",
       "      <td>0.06221</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.05912</td>\n",
       "      <td>...</td>\n",
       "      <td>14.670</td>\n",
       "      <td>16.93</td>\n",
       "      <td>94.17</td>\n",
       "      <td>661.1</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.03732</td>\n",
       "      <td>0.05802</td>\n",
       "      <td>0.2823</td>\n",
       "      <td>0.06794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>19.100</td>\n",
       "      <td>26.29</td>\n",
       "      <td>129.10</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>0.12150</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>0.07224</td>\n",
       "      <td>...</td>\n",
       "      <td>20.330</td>\n",
       "      <td>32.72</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.28170</td>\n",
       "      <td>0.24320</td>\n",
       "      <td>0.18410</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.09203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>13.050</td>\n",
       "      <td>13.84</td>\n",
       "      <td>82.71</td>\n",
       "      <td>530.6</td>\n",
       "      <td>0.08352</td>\n",
       "      <td>0.03735</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.05518</td>\n",
       "      <td>...</td>\n",
       "      <td>14.730</td>\n",
       "      <td>17.40</td>\n",
       "      <td>93.96</td>\n",
       "      <td>672.4</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>0.05847</td>\n",
       "      <td>0.01824</td>\n",
       "      <td>0.03532</td>\n",
       "      <td>0.2107</td>\n",
       "      <td>0.06580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.040</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>8.878</td>\n",
       "      <td>15.49</td>\n",
       "      <td>56.74</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.08293</td>\n",
       "      <td>0.07698</td>\n",
       "      <td>0.047210</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.06621</td>\n",
       "      <td>...</td>\n",
       "      <td>9.981</td>\n",
       "      <td>17.70</td>\n",
       "      <td>65.27</td>\n",
       "      <td>302.0</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>0.12480</td>\n",
       "      <td>0.09441</td>\n",
       "      <td>0.04762</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.07431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>11.280</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.046350</td>\n",
       "      <td>0.047960</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.06072</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.18220</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "86        14.480         21.46           94.25      648.2          0.09444   \n",
       "295       13.770         13.27           88.06      582.7          0.09198   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "83        19.100         26.29          129.10     1132.0          0.12150   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "309       13.050         13.84           82.71      530.6          0.08352   \n",
       "435       13.980         19.62           91.12      599.5          0.10600   \n",
       "358        8.878         15.49           56.74      241.0          0.08293   \n",
       "139       11.280         13.39           73.00      384.8          0.11640   \n",
       "7         13.710         20.83           90.20      577.9          0.11890   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
       "86            0.09947        0.120400             0.049380         0.2075   \n",
       "295           0.06221        0.010630             0.019170         0.1592   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "83            0.17910        0.193700             0.146900         0.1634   \n",
       "..                ...             ...                  ...            ...   \n",
       "309           0.03735        0.004559             0.008829         0.1453   \n",
       "435           0.11330        0.112600             0.064630         0.1669   \n",
       "358           0.07698        0.047210             0.023810         0.1930   \n",
       "139           0.11360        0.046350             0.047960         0.1771   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "86                  0.05636  ...        16.210          29.25   \n",
       "295                 0.05912  ...        14.670          16.93   \n",
       "562                 0.07152  ...        17.520          42.79   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "83                  0.07224  ...        20.330          32.72   \n",
       "..                      ...  ...           ...            ...   \n",
       "309                 0.05518  ...        14.730          17.40   \n",
       "435                 0.06544  ...        17.040          30.80   \n",
       "358                 0.06621  ...         9.981          17.70   \n",
       "139                 0.06072  ...        11.920          15.77   \n",
       "7                   0.07451  ...        17.060          28.14   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "86            108.40       808.9            0.1306            0.19760   \n",
       "295            94.17       661.1            0.1170            0.10720   \n",
       "562           128.70       915.0            0.1417            0.79170   \n",
       "565           155.00      1731.0            0.1166            0.19220   \n",
       "83            141.30      1298.0            0.1392            0.28170   \n",
       "..               ...         ...               ...                ...   \n",
       "309            93.96       672.4            0.1016            0.05847   \n",
       "435           113.90       869.3            0.1613            0.35680   \n",
       "358            65.27       302.0            0.1015            0.12480   \n",
       "139            76.53       434.0            0.1367            0.18220   \n",
       "7             110.60       897.0            0.1654            0.36820   \n",
       "\n",
       "     concavity_worst  concave_points_worst  symmetry_worst  \\\n",
       "86           0.33490               0.12250          0.3020   \n",
       "295          0.03732               0.05802          0.2823   \n",
       "562          1.17000               0.23560          0.4089   \n",
       "565          0.32150               0.16280          0.2572   \n",
       "83           0.24320               0.18410          0.2311   \n",
       "..               ...                   ...             ...   \n",
       "309          0.01824               0.03532          0.2107   \n",
       "435          0.40690               0.18270          0.3179   \n",
       "358          0.09441               0.04762          0.2434   \n",
       "139          0.08669               0.08611          0.2102   \n",
       "7            0.26780               0.15560          0.3196   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "86                   0.06846  \n",
       "295                  0.06794  \n",
       "562                  0.14090  \n",
       "565                  0.06637  \n",
       "83                   0.09203  \n",
       "..                       ...  \n",
       "309                  0.06580  \n",
       "435                  0.10550  \n",
       "358                  0.07431  \n",
       "139                  0.06784  \n",
       "7                    0.11510  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09be3bd5-b99b-498a-ae1a-a9dc37da9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logisitic Regression : 95.32%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=5, stratify=y)\n",
    "\n",
    "clf = LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Accuracy of Logisitic Regression : {accuracy_score(y_pred,y_test)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d75269f1-adb3-4515-89de-d49ae683d67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>15.71</td>\n",
       "      <td>13.93</td>\n",
       "      <td>102.00</td>\n",
       "      <td>761.7</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>0.07135</td>\n",
       "      <td>0.059330</td>\n",
       "      <td>0.1816</td>\n",
       "      <td>0.05723</td>\n",
       "      <td>...</td>\n",
       "      <td>17.50</td>\n",
       "      <td>19.25</td>\n",
       "      <td>114.30</td>\n",
       "      <td>922.8</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.19490</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.2723</td>\n",
       "      <td>0.07071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>12.34</td>\n",
       "      <td>22.22</td>\n",
       "      <td>79.85</td>\n",
       "      <td>464.5</td>\n",
       "      <td>0.10120</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.05370</td>\n",
       "      <td>0.028220</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.06761</td>\n",
       "      <td>...</td>\n",
       "      <td>13.58</td>\n",
       "      <td>28.68</td>\n",
       "      <td>87.36</td>\n",
       "      <td>553.0</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.23380</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.081940</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.09082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.82</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.01548</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>13.03</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.061940</td>\n",
       "      <td>0.032640</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>12.75</td>\n",
       "      <td>16.70</td>\n",
       "      <td>82.51</td>\n",
       "      <td>493.8</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.11170</td>\n",
       "      <td>0.03880</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.06623</td>\n",
       "      <td>...</td>\n",
       "      <td>14.45</td>\n",
       "      <td>21.74</td>\n",
       "      <td>93.63</td>\n",
       "      <td>624.1</td>\n",
       "      <td>0.1475</td>\n",
       "      <td>0.19790</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.080450</td>\n",
       "      <td>0.3071</td>\n",
       "      <td>0.08557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.88</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.06195</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.89</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.1227</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>12.58</td>\n",
       "      <td>18.40</td>\n",
       "      <td>79.83</td>\n",
       "      <td>489.0</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05855</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>23.08</td>\n",
       "      <td>85.56</td>\n",
       "      <td>564.1</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.06624</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.06431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>13.17</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.28</td>\n",
       "      <td>537.3</td>\n",
       "      <td>0.07466</td>\n",
       "      <td>0.05994</td>\n",
       "      <td>0.04859</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.05549</td>\n",
       "      <td>...</td>\n",
       "      <td>14.90</td>\n",
       "      <td>23.89</td>\n",
       "      <td>95.10</td>\n",
       "      <td>687.6</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.19650</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>14.50</td>\n",
       "      <td>10.89</td>\n",
       "      <td>94.28</td>\n",
       "      <td>640.7</td>\n",
       "      <td>0.11010</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.057780</td>\n",
       "      <td>0.1856</td>\n",
       "      <td>0.06402</td>\n",
       "      <td>...</td>\n",
       "      <td>15.70</td>\n",
       "      <td>15.98</td>\n",
       "      <td>102.80</td>\n",
       "      <td>745.5</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.17880</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.2889</td>\n",
       "      <td>0.08006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>18.03</td>\n",
       "      <td>16.85</td>\n",
       "      <td>117.50</td>\n",
       "      <td>990.0</td>\n",
       "      <td>0.08947</td>\n",
       "      <td>0.12320</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.062540</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.05780</td>\n",
       "      <td>...</td>\n",
       "      <td>20.38</td>\n",
       "      <td>22.02</td>\n",
       "      <td>133.30</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.26660</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.2842</td>\n",
       "      <td>0.08225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "133        15.71         13.93          102.00      761.7          0.09462   \n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "163        12.34         22.22           79.85      464.5          0.10120   \n",
       "549        10.82         24.21           68.89      361.6          0.08192   \n",
       "519        12.75         16.70           82.51      493.8          0.11250   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "554        12.88         28.92           82.50      514.3          0.08123   \n",
       "285        12.58         18.40           79.83      489.0          0.08393   \n",
       "464        13.17         18.22           84.28      537.3          0.07466   \n",
       "123        14.50         10.89           94.28      640.7          0.11010   \n",
       "444        18.03         16.85          117.50      990.0          0.08947   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
       "133           0.09462         0.07135             0.059330         0.1816   \n",
       "0             0.27760         0.30010             0.147100         0.2419   \n",
       "163           0.10150         0.05370             0.028220         0.1551   \n",
       "549           0.06602         0.01548             0.008160         0.1976   \n",
       "519           0.11170         0.03880             0.029950         0.2120   \n",
       "..                ...             ...                  ...            ...   \n",
       "554           0.05824         0.06195             0.023430         0.1566   \n",
       "285           0.04216         0.00186             0.002924         0.1697   \n",
       "464           0.05994         0.04859             0.028700         0.1454   \n",
       "123           0.10990         0.08842             0.057780         0.1856   \n",
       "444           0.12320         0.10900             0.062540         0.1720   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "133                 0.05723  ...         17.50          19.25   \n",
       "0                   0.07871  ...         25.38          17.33   \n",
       "163                 0.06761  ...         13.58          28.68   \n",
       "549                 0.06328  ...         13.03          31.45   \n",
       "519                 0.06623  ...         14.45          21.74   \n",
       "..                      ...  ...           ...            ...   \n",
       "554                 0.05708  ...         13.89          35.74   \n",
       "285                 0.05855  ...         13.50          23.08   \n",
       "464                 0.05549  ...         14.90          23.89   \n",
       "123                 0.06402  ...         15.70          15.98   \n",
       "444                 0.05780  ...         20.38          22.02   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "133           114.30       922.8            0.1223            0.19490   \n",
       "0             184.60      2019.0            0.1622            0.66560   \n",
       "163            87.36       553.0            0.1452            0.23380   \n",
       "549            83.90       505.6            0.1204            0.16330   \n",
       "519            93.63       624.1            0.1475            0.19790   \n",
       "..               ...         ...               ...                ...   \n",
       "554            88.84       595.7            0.1227            0.16200   \n",
       "285            85.56       564.1            0.1038            0.06624   \n",
       "464            95.10       687.6            0.1282            0.19650   \n",
       "123           102.80       745.5            0.1313            0.17880   \n",
       "444           133.30      1292.0            0.1263            0.26660   \n",
       "\n",
       "     concavity_worst  concave_points_worst  symmetry_worst  \\\n",
       "133         0.170900              0.137400          0.2723   \n",
       "0           0.711900              0.265400          0.4601   \n",
       "163         0.168800              0.081940          0.2268   \n",
       "549         0.061940              0.032640          0.3059   \n",
       "519         0.142300              0.080450          0.3071   \n",
       "..               ...                   ...             ...   \n",
       "554         0.243900              0.064930          0.2372   \n",
       "285         0.005579              0.008772          0.2505   \n",
       "464         0.187600              0.104500          0.2235   \n",
       "123         0.256000              0.122100          0.2889   \n",
       "444         0.429000              0.153500          0.2842   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "133                  0.07071  \n",
       "0                    0.11890  \n",
       "163                  0.09082  \n",
       "549                  0.07626  \n",
       "519                  0.08557  \n",
       "..                       ...  \n",
       "554                  0.07242  \n",
       "285                  0.06431  \n",
       "464                  0.06925  \n",
       "123                  0.08006  \n",
       "444                  0.08225  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d42db8b-b0fd-4d6a-8a8b-9b332b17055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logisitic Regression : 97.08%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=12, stratify=y)\n",
    "\n",
    "clf = LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Accuracy of Logisitic Regression : {accuracy_score(y_pred,y_test)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d3d4783-d00c-4764-a5d2-1114ca4a3ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>13.610</td>\n",
       "      <td>24.98</td>\n",
       "      <td>88.05</td>\n",
       "      <td>582.7</td>\n",
       "      <td>0.09488</td>\n",
       "      <td>0.08511</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>0.044890</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.05871</td>\n",
       "      <td>...</td>\n",
       "      <td>16.990</td>\n",
       "      <td>35.27</td>\n",
       "      <td>108.60</td>\n",
       "      <td>906.5</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.19430</td>\n",
       "      <td>0.31690</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.07397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>12.490</td>\n",
       "      <td>16.85</td>\n",
       "      <td>79.19</td>\n",
       "      <td>481.6</td>\n",
       "      <td>0.08511</td>\n",
       "      <td>0.03834</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>0.1215</td>\n",
       "      <td>0.05673</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340</td>\n",
       "      <td>19.71</td>\n",
       "      <td>84.48</td>\n",
       "      <td>544.2</td>\n",
       "      <td>0.11040</td>\n",
       "      <td>0.04953</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.02784</td>\n",
       "      <td>0.1917</td>\n",
       "      <td>0.06174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13.480</td>\n",
       "      <td>20.82</td>\n",
       "      <td>88.40</td>\n",
       "      <td>559.2</td>\n",
       "      <td>0.10160</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.054390</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.06419</td>\n",
       "      <td>...</td>\n",
       "      <td>15.530</td>\n",
       "      <td>26.02</td>\n",
       "      <td>107.30</td>\n",
       "      <td>740.4</td>\n",
       "      <td>0.16100</td>\n",
       "      <td>0.42250</td>\n",
       "      <td>0.50300</td>\n",
       "      <td>0.22580</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.10710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>11.800</td>\n",
       "      <td>17.26</td>\n",
       "      <td>75.26</td>\n",
       "      <td>431.9</td>\n",
       "      <td>0.09087</td>\n",
       "      <td>0.06232</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.06019</td>\n",
       "      <td>...</td>\n",
       "      <td>13.450</td>\n",
       "      <td>24.49</td>\n",
       "      <td>86.00</td>\n",
       "      <td>562.0</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.17260</td>\n",
       "      <td>0.14490</td>\n",
       "      <td>0.05356</td>\n",
       "      <td>0.2779</td>\n",
       "      <td>0.08121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>12.360</td>\n",
       "      <td>18.54</td>\n",
       "      <td>79.01</td>\n",
       "      <td>466.7</td>\n",
       "      <td>0.08477</td>\n",
       "      <td>0.06815</td>\n",
       "      <td>0.026430</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>0.1602</td>\n",
       "      <td>0.06066</td>\n",
       "      <td>...</td>\n",
       "      <td>13.290</td>\n",
       "      <td>27.49</td>\n",
       "      <td>85.56</td>\n",
       "      <td>544.1</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.19630</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.08442</td>\n",
       "      <td>0.2983</td>\n",
       "      <td>0.07185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>0.016150</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>0.06329</td>\n",
       "      <td>...</td>\n",
       "      <td>12.250</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.06136</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>0.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.06963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.010</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "100       13.610         24.98           88.05      582.7          0.09488   \n",
       "315       12.490         16.85           79.19      481.6          0.08511   \n",
       "39        13.480         20.82           88.40      559.2          0.10160   \n",
       "553        9.333         21.94           59.01      264.0          0.09240   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "399       11.800         17.26           75.26      431.9          0.09087   \n",
       "107       12.360         18.54           79.01      466.7          0.08477   \n",
       "248       10.650         25.22           68.01      347.0          0.09657   \n",
       "63         9.173         13.86           59.20      260.9          0.07721   \n",
       "559       11.510         23.93           74.52      403.5          0.09261   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "100           0.08511        0.086250             0.044890         0.1609   \n",
       "315           0.03834        0.004473             0.006423         0.1215   \n",
       "39            0.12550        0.106300             0.054390         0.1720   \n",
       "553           0.05605        0.039960             0.012820         0.1692   \n",
       "..                ...             ...                  ...            ...   \n",
       "399           0.06232        0.028530             0.016380         0.1847   \n",
       "107           0.06815        0.026430             0.019210         0.1602   \n",
       "248           0.07234        0.023790             0.016150         0.1897   \n",
       "63            0.08751        0.059880             0.021800         0.2341   \n",
       "559           0.10210        0.111200             0.041050         0.1388   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "552                 0.05637  ...        13.870          36.00   \n",
       "100                 0.05871  ...        16.990          35.27   \n",
       "315                 0.05673  ...        13.340          19.71   \n",
       "39                  0.06419  ...        15.530          26.02   \n",
       "553                 0.06576  ...         9.845          25.05   \n",
       "..                      ...  ...           ...            ...   \n",
       "399                 0.06019  ...        13.450          24.49   \n",
       "107                 0.06066  ...        13.290          27.49   \n",
       "248                 0.06329  ...        12.250          35.19   \n",
       "63                  0.06963  ...        10.010          19.23   \n",
       "559                 0.06570  ...        12.480          37.16   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "552            88.10       594.7           0.12340            0.10640   \n",
       "100           108.60       906.5           0.12650            0.19430   \n",
       "315            84.48       544.2           0.11040            0.04953   \n",
       "39            107.30       740.4           0.16100            0.42250   \n",
       "553            62.86       295.8           0.11030            0.08298   \n",
       "..               ...         ...               ...                ...   \n",
       "399            86.00       562.0           0.12440            0.17260   \n",
       "107            85.56       544.1           0.11840            0.19630   \n",
       "248            77.98       455.7           0.14990            0.13980   \n",
       "63             65.59       310.1           0.09836            0.16780   \n",
       "559            82.28       474.2           0.12980            0.25170   \n",
       "\n",
       "     concavity_worst  concave_points_worst  symmetry_worst  \\\n",
       "552          0.08653               0.06498          0.2407   \n",
       "100          0.31690               0.11840          0.2651   \n",
       "315          0.01938               0.02784          0.1917   \n",
       "39           0.50300               0.22580          0.2807   \n",
       "553          0.07993               0.02564          0.2435   \n",
       "..               ...                   ...             ...   \n",
       "399          0.14490               0.05356          0.2779   \n",
       "107          0.19370               0.08442          0.2983   \n",
       "248          0.11250               0.06136          0.3409   \n",
       "63           0.13970               0.05087          0.3282   \n",
       "559          0.36300               0.09653          0.2112   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "552                  0.06484  \n",
       "100                  0.07397  \n",
       "315                  0.06174  \n",
       "39                   0.10710  \n",
       "553                  0.07393  \n",
       "..                       ...  \n",
       "399                  0.08121  \n",
       "107                  0.07185  \n",
       "248                  0.08147  \n",
       "63                   0.08490  \n",
       "559                  0.08732  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b34aa-46f4-447b-9acf-6d9eee2ace6d",
   "metadata": {},
   "source": [
    "## Cross Validation:\n",
    "\n",
    "-     How could u say that ML Model predicts xx.yy% as Accuracy for any given Test Sample from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ff89b4f-0ced-42a2-881e-cd5ef4435940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(cross_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c42966-52fe-40df-b4c1-a5911825670d",
   "metadata": {},
   "source": [
    "### 1.Call cross_val_score with cv value\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9087ad58-88f1-4796-ad8e-8115a78a5449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logisitic Regression  : 95.96% +- 1.31%\n",
      "Precision of Logisitic Regression : 94.90% +- 2.17%\n",
      "Recall of Logisitic Regression    : 94.84% +- 2.69%\n",
      "F1-Score of Logisitic Regression  : 94.84% +- 1.76%\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "print(f'Accuracy of Logisitic Regression  : {scores.mean()*100:0.2f}% +- {scores.std()*100:0.2f}%')\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='precision')\n",
    "print(f'Precision of Logisitic Regression : {scores.mean()*100:0.2f}% +- {scores.std()*100:0.2f}%')\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='recall')\n",
    "print(f'Recall of Logisitic Regression    : {scores.mean()*100:0.2f}% +- {scores.std()*100:0.2f}%')\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n",
    "print(f'F1-Score of Logisitic Regression  : {scores.mean()*100:0.2f}% +- {scores.std()*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacadda-cf22-4970-91b6-32c3736f9c79",
   "metadata": {},
   "source": [
    "### 2. Different Splitter   KFold/StratifiedKFold for cross validation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec538947-82e8-41bd-bcd3-2919f63a6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logisitic Regression : 96.48% +- 2.02%\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n",
    "print(f'Accuracy of Logisitic Regression : {scores.mean()*100:0.2f}% +- {scores.std()*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507cb73-6929-4c5d-bbdb-4450061356b9",
   "metadata": {},
   "source": [
    "### Metric to measure Classification : \n",
    "        - Confusion Matrix\n",
    "        - Accuracy\n",
    "        - Precision\n",
    "        - Recall\n",
    "        - F1-Score\n",
    "        - Classification Report\n",
    "        - AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a8a92-3d8e-4fe9-b3a5-601af454f8b4",
   "metadata": {},
   "source": [
    "### Confusuon Matrix \n",
    "    - Binary Class : [0,1] : SIZE (2,2)\n",
    "    - Multi-Clas : \n",
    "        [0,1,2] : SIZE (3,3)\n",
    "        [0,1,2,3] : SIZE (4,4)\n",
    "        [0,1,2,3,4] : SIZE (5,5)\n",
    "\n",
    "        Actual Value (y_test)      :    0            |    1\n",
    "                                   -------------------------------------\n",
    "        Prediction   (y_pred)   0  |  True Negative  |  False Negative | \n",
    "                                   -------------------------------------\n",
    "                                1  |  False Positive |  True Positive  |\n",
    "                                   -------------------------------------\n",
    "\n",
    "### Accuracy: The proportion of actual correct prediction\n",
    "        Accuracy     = (TP + TN) / (TP + FP + TN + FN)\n",
    "### Precision: The proportion of predicted positive classes was actually positive \n",
    "        Precision   =  TP  / ( TP+FP)\n",
    "### Sensitivity/Recall: The proportion of actual positive classes was predicted correctly \n",
    "        Recall       =  TP / (TP + FN)\n",
    "### Specificity: The proportion of actual negative classes was predicted correctly. \n",
    "        Specificity = TN / (TN + FP)\n",
    "### Fscore: f1-scores closer to 1 are a better model because balance precision and recall.\n",
    "        F1-Score     =  2 * Recall * Precision / (Recall + Precision)\n",
    "\n",
    "### AUC_ROC (Area Under Receiver operating characteristics curve)\n",
    "    - for : False Positive Rate\n",
    "    - tpr : True Positive Rate \n",
    "    - roc_auc_score : \n",
    "    - roc_curve : \n",
    "    \n",
    "### Classification Report: Summary of Each Class Precision, Recall, Support Sampes and Accuracy and F1-Scores (macro, micro,weighted)\n",
    "\n",
    "https://www.aitude.com/8-important-evaluation-metrics-for-classification-models/\n",
    "\n",
    "\n",
    "https://www.v7labs.com/blog/f1-score-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b07d0b0c-321c-4184-bd21-546c61307a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=42, stratify=y)\n",
    "clf = LogisticRegression(C=10.0, penalty='l1', solver='liblinear', tol=0.001)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f3b0441-fef9-4961-b032-f7cd0aa1de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[105,   2],\n",
       "       [  8,  56]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Confusion Matrix:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f716ab34-3c57-4a93-b5a3-290bfd56af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.942\n",
      "Precision : 0.966\n",
      "Recall    : 0.875\n",
      "F1-Score  : 0.918\n",
      "ROC Score : 0.928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       107\n",
      "           1       0.97      0.88      0.92        64\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.95      0.93      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy  : {accuracy_score(y_test, y_pred):0.3f}')\n",
    "print(f'Precision : {precision_score(y_test, y_pred):0.3f}')\n",
    "print(f'Recall    : {recall_score(y_test, y_pred):0.3f}')\n",
    "print(f'F1-Score  : {f1_score(y_test, y_pred):0.3f}')\n",
    "print(f'ROC Score : {roc_auc_score(y_test,y_pred):0.3f}')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0171f758-5eb8-416e-affe-53ba852ecc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99818454, 0.00181546]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsample = np.reshape(X.loc[50].values, (-1, 30))\n",
    "clf.predict_proba(newsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21144506-ac2a-4d88-8c12-d551dc4f0195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_probs = clf.predict_proba(X)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:,1]\n",
    "lr_auc = roc_auc_score(y, lr_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y, lr_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
