1. Parameter Optimization (SVM ( default parameters values change ) GridSearch

2. Pipelines (Scaling, Kernel/Model) : Purpose

3. Cross Validation: (Classifier avg performance out of all the samples )
    Divided our training and testing dataset multiple times (5 fold cross Validation, 10 fold cross, 2....10)
    - cross_val_score(X=X,y=y,cv=5) # cross validation by using number of fold = 5
    - cv= KFold(nspliter=5) : cross_val_score(X=X,y=y, cv=cv)
    - cv = RepeatedKfold(nspliter=5) ,

4. Penalties L1/L2

Feature Selection :
    -> Univariate Feature selection (SelectKBest)
    -> Recursive Feature selection (RFE)
    -> Principal component Analysis (PCA) : Dimentionality Reduction (1000,100) =? (1000,10) 
    -> Feature Importance (Random forest Classifer / ExtraTrees Classifer)

Regression : 
    -> LinearRegression
    -> LogisticRegression(Classification) => Linear Regression + Sigmoid
    -> Lasso Regression L1
    -> Ridge Regression L2
    -> SVM( Support Vector Regressor (SVR)
    -> Decision Tree Regressor 

Classification :
    -> LogisticRegression
    -> KNearestNeighbor
    -> Naive Bayes (Bayes Algorithm)
    -> SVM (Linear/NonLinear) (linear/poly/sigmoid/rbf) 
    -> Decision Tree Classifier (entropy/giniindex)
    -> Ensemble models:

    Datasets = 1000 sample
    Estimator = Decision Tree . 100
    Training Data = 600 samples 400 testing 
        -> Bagging Classifier : Random Forest / Extra Tree Classifier
        -> Boosting Classifier: AdaBoost, XtreamBoost 
        -> Voting Classifier

Unsupervised Learning Algorithms:
    - Similaries with in data : Clustering
    - Someone does something vs relation with between two sample : Association 
    
    Clustering :
        Partition based Clustering :
            Kmeans : (How many clusters you want to build) = 2,3,4,5,6,7
                1. it will randomly select the centroid of the cluster n clusters
                    (xmin,ymin) (xmax,ymax)
                2. For each points, you need to compute the distance with centroid, which 
                   one is having minimum distance, you will assign the point that cluster
                3. Update the centroid of the cluster
                Points (100 ) ( (x1,y1), (x2,y2).....(x100,y100))
            Kmeans++:
                1.  random sample point as cluster centroid 
                    (xmin,ymin) (xmax,ymax)
        Hierarchical Clusteing:
            - Agglomerative Clustering
            - Divisive Clustering

# Pipeline in ML

A pipeline in machine learning is a sequence of data processing steps that are chained together. The pipeline is designed to take in raw data and produce a final model that can make predictions on new data. The main purpose of a pipeline is to automate the machine learning workflow and make the process of building models faster and more efficient.

## Steps to Create a Pipeline

1. **Data Preprocessing:** The first step is to preprocess the data by cleaning and transforming it. This includes handling missing values, scaling the data, and encoding categorical variables.
2. **Feature Engineering:** The next step is to engineer the features to create new and more meaningful variables. This can include dimensionality reduction, feature selection, and feature extraction.
3. **Model Selection:** The next step is to select the appropriate model for the problem at hand. This can include regression, classification, and clustering models.
4. **Hyperparameter Tuning:** Once the model is selected, the next step is to tune the hyperparameters to improve the model's performance. This involves trying different combinations of hyperparameters and evaluating the model's performance on a validation set.
5. **Model Training:** After the hyperparameters are tuned, the next step is to train the model on the entire training set.
6. **Model Evaluation:** The final step is to evaluate the performance of the model on a test set. This involves calculating metrics such as accuracy, precision, and recall.

## Putting It All Together

To put it all together, we can create a pipeline using the Scikit-Learn library in Python. Here's an example:

```
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),
    ('classifier', RandomForestClassifier())
])

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('RandomForest', RandomForestClassifier())
])
```

In this example, the pipeline has four steps:

1. **Imputing missing values:** We use the SimpleImputer class to fill in missing values with the median of each column.
2. **Scaling the data:** We use the StandardScaler class to scale the data to zero mean and unit variance.
3. **Performing PCA:** We use the PCA class to perform dimensionality reduction and create 10 new features.
4. **Training a Random Forest Classifier:** We use the RandomForestClassifier class to train a classification model.

## Conclusion

In conclusion, creating a pipeline in machine learning can streamline the process of building models and make it easier to experiment with different techniques. By following the steps outlined above and using the Scikit-Learn library in Python, you can easily create a pipeline that preprocesses the data, engineers new features, selects a model, tunes the hyperparameters, trains the model, and evaluates its performance.
